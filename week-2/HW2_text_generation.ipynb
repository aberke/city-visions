{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Generative Models\n",
    "\n",
    "In this assignment we will see some generative models for text: CharRNN, Transformers and Chatbots. Training text models is very time consuming, and uses a ton of data. The really good models also tend to be very large, so we will stick to pretrained models. Those can still be excellent to generate totally new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Embeddings are numeric representations for non-numeric data. In our case we look for embeddings for words. A simple kind of embedding is One-Hot Encoding, where we put a `1` in a vector of all `0`s at the index of the word in the vocabulary.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "But that can be very wasteful and also doesn't encode any relationship between the words.\n",
    "\n",
    "To learn semantic relationship a few unsupervised algorithms were proposed. In class we've discussed Continuous Bag of Words and Skip-Gram. Essentially these will mask out part of a sentence and ask the model to predict the missing part. This way the model learns about the context words are used in sentences as well as relationships.\n",
    "\n",
    "Embedding for a word is a vector of numbers:\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" width=\"50%\" />\n",
    "\n",
    "Luckily many of the world leaders in natural language processing have pretrained word embeddings learned on huge corpora, so we don't have to do it ourselves.\n",
    "\n",
    "Allison Parrish of NYU showed some very interesting uses for word embeddings for poetry generation: https://www.youtube.com/watch?v=L3D0JEA1Jdc breeze through this StrangeLoop talk for inspiration. I encourage you to try these methods towards you own generative work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chakin` is a helper tool for downloading pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q chakin progressbar2 textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chakin\n",
    "import progressbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search('English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download GLoVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:06:27   2.1 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./glove.6B.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chakin.download(number=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need one file (the smallest dimension one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the embedding values for each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n"
     ]
    }
   ],
   "source": [
    "!head -5 glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load them into memory and organize a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_lines = open('glove.6B.50d.txt','rt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100000 of 100000) |################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "w2v_emb_dict = dict()\n",
    "pbar = progressbar.ProgressBar(max_value=100000)\n",
    "for i,l in enumerate(w2vec_lines[1:100000]):\n",
    "    w,emb = l.split(' ', 1)\n",
    "    w2v_emb_dict[w] = np.fromstring(emb, sep=' ')\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would be the first most commonly used tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authoritatively',\n",
       " '1,520',\n",
       " 'quinton',\n",
       " 'editorialized',\n",
       " 'beutel',\n",
       " 'gashes',\n",
       " 'pronounced',\n",
       " 'bettered',\n",
       " 'jagdish',\n",
       " 'eglin']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w2v_emb_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies and Similarities\n",
    "\n",
    "Embeddings carry semantic information in their numeric encoding. Exploring this semantic space can be fun, for example looking for similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is measuring the angle between vectors. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2432/1*Acs3Kbrrrb4d3fqMlGhMcQ.png\"/>\n",
    "\n",
    "Our embeddings are normalized vectors so looking at the angle between two vectors reveals how far away they are from one another in the high-dimensional embdding space:\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "w2v_emb_dict_keys = list(w2v_emb_dict.keys())\n",
    "w2v_emb_dict_values = np.array(list(w2v_emb_dict.values()))\n",
    "\n",
    "def find_nearest(w):\n",
    "    return w2v_emb_dict_keys[cosine_similarity(w2v_emb_dict[w].reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-2]]\n",
    "def find_nearest_top_k(v, k=5):\n",
    "    return [w2v_emb_dict_keys[w] for w in cosine_similarity(v.reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-k:].tolist()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at closest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaching'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider \"**word analogies**\", e.g. completing the sentence: \"Paris is to France like Rome is to ___\" \\(Italy\\)\n",
    "\n",
    "To explain this geometrically:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2632/1*EOVxNmHkrsPQ7Q44N0OiQg.png\" width=\"60%\" />\n",
    "\n",
    "The offset vector between \"paris\" and \"france\" is the \"Captial of\" vector, and when we apply it to \"rome\" we expect to get \"italy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italy', 'spain', 'rome', 'portugal', 'france']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['france'] - w2v_emb_dict['paris'] + w2v_emb_dict['rome'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'daughter', 'prince', 'throne']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['king'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following analogies:\n",
    "1. sushi-rice is like pizza-___\n",
    "2. sushi-rice is like steak-___\n",
    "3. shirt-clothing is like phone-___\n",
    "4. shirt-clothing is like bowl-___\n",
    "5. book-reading is like TV-___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. sushi-rice is like pizza-___\n",
      "['pizza', 'sushi', 'fast-food', 'diner', 'nachos']\n"
     ]
    }
   ],
   "source": [
    "print('1. sushi-rice is like pizza-___')\n",
    "print(find_nearest_top_k(w2v_emb_dict['sushi'] - w2v_emb_dict['rice'] + w2v_emb_dict['pizza'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['steak', 'sushi', 'cheeseburger', 'steaks', 'meatball']\n"
     ]
    }
   ],
   "source": [
    "# 2. sushi-rice is like steak-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['sushi'] - w2v_emb_dict['rice'] + w2v_emb_dict['steak'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza', 'sandwich', 'hat', 'sandwiches', 'pie']\n"
     ]
    }
   ],
   "source": [
    "# 3. shirt-clothing is like phone-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['shirt'] - w2v_emb_dict['clothing'] + w2v_emb_dict['pizza'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bowl', 'crimson', 'afc', 'gator', 'super']\n"
     ]
    }
   ],
   "source": [
    "# 4. shirt-clothing is like bowl-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['shirt'] - w2v_emb_dict['clothing'] + w2v_emb_dict['bowl'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'television', 'movie', 'hbo', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# 5. book-reading is like TV-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['book'] - w2v_emb_dict['reading'] + w2v_emb_dict['tv'], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find analogies that don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing around to find analogies that do not work…\n",
    "\n",
    "Breakfast-morning is like dinner-____\n",
    "\n",
    "Breakfast-morning is like lunch-_____\n",
    "\n",
    "Pie-dessert is like broccoli-____\n",
    "\n",
    "cake-dessert is like spinach-____\n",
    "\n",
    "Ketchup-burger is like syrup-_____\n",
    "\n",
    "Hummus-carrots is like mustard-____\n",
    "\n",
    "Art-paint is like literature-_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dinners', 'breakfast', 'dinner', 'buffet', 'breakfasts']\n"
     ]
    }
   ],
   "source": [
    "# Breakfast-morning is like dinner-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['breakfast'] - w2v_emb_dict['morning'] + w2v_emb_dict['dinner'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breakfast', 'buffet', 'breakfasts', 'dinners', 'lunch']\n"
     ]
    }
   ],
   "source": [
    "# Breakfast-morning is like lunch-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['breakfast'] - w2v_emb_dict['morning'] + w2v_emb_dict['lunch'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['broccoli', 'cauliflower', 'zucchini', 'sprouts', 'celery']\n"
     ]
    }
   ],
   "source": [
    "# Pie-dessert is like broccoli-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['pie'] - w2v_emb_dict['dessert'] + w2v_emb_dict['broccoli'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spinach', 'lettuce', 'potatoes', 'peeled', 'carrots']\n"
     ]
    }
   ],
   "source": [
    "# Icecream-dessert is like spinach-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['cake'] - w2v_emb_dict['dessert'] + w2v_emb_dict['spinach'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syrup', 'vinegar', 'molasses', 'juice', 'vanilla']\n"
     ]
    }
   ],
   "source": [
    "# Ketchup-burger is like syrup-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['ketchup'] - w2v_emb_dict['burger'] + w2v_emb_dict['syrup'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hummus', 'naim', 'malak', 'ales', 'kassem']\n"
     ]
    }
   ],
   "source": [
    "# Hummus-carrots is like mustard-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['hummus'] - w2v_emb_dict['carrots'] + w2v_emb_dict['mustard'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['literature', 'literary', 'poetry', 'scholar', 'contemporary']\n"
     ]
    }
   ],
   "source": [
    "# Art-paint is like literature-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['art'] - w2v_emb_dict['paint'] + w2v_emb_dict['literature'], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN\n",
    "\n",
    "CharRNN is a simple recurrent neural network architecture that works on the character level (not words). It's surprisingly powerful at generating text. These were popularized by [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"50%\"/>\n",
    "\n",
    "The `textgenrnn` package is a convnient way to train and generate with CharRNNs. Here we're using its built in model. They have multiple models [published](https://github.com/minimaxir/textgenrnn/tree/master/weights) trained on different corpora.\n",
    "\n",
    "People created some very cool projects with it: https://github.com/minimaxir/textgenrnn#projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1025 18:29:09.660571 139840947357440 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key confirmed for a random burger to be on the floor?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supply a prefix to prime the model with text to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When life gives you lemons as a month of the company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"When life gives you lemons \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also let the model try different \"temperatures\". The \"temperature\" controls the level of random choice when picking the next character, instead of always the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "[Specific] Can someone please remove the story of the streets and the most support is the best computer and why are the posts of the state of the best community to the post in the top of the state of the state of the same time to a super of the state of the state of the state of the starter of the \n",
      "\n",
      "[Specific] Can someone please remove the story of the programming in the most state of the state of the most planet in the state of the season in the back of the same things to get a girl to the strange with a huge character and become still going to be a man who has a stranger to the programming t\n",
      "\n",
      "The subreddit of the state of the state of the discovery of the same state of the same starting to the state of the first time that we say they are a big defender of the story of the state of the state of the state of the state of the story of the streets of the state of the same time in the first \n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "[Humor] I just learned to the best part of the best room of the best fun and I am there as he for the best for me for you.\n",
      "\n",
      "A study got me a base my Facebook that looks like a man who seemingly appreciate it.\"\n",
      "\n",
      "I want to complete the conversation of the team to the most game of the world is simple with the top of the basic crime is back to standard in the Anchor of the Draft Save and Morty - an Advice of the 'Senators' Christmas that can remove the good time in a time in the World friend who says he was s\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Thank you Morty, PsBattle: orteuj makes me when they saw \"The Get Hard\"\n",
      "\n",
      "I work for which Reddit with your Monu, I've been jacked out the Repaire Book Caster [F]\n",
      "\n",
      "Bigs spia 'man's \"scared after learning cards\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefixes and temperatures. (examine the `.generate()` function, by running a cell with `textgenrnn.generate?`)\n",
    "* Try a different pretrained model from `textgenrnn`\n",
    "* Advanced: train your own model! `textgenrnn` provide a **very** simple mechanism to do so: https://github.com/minimaxir/textgenrnn#examples, you just need to supply a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtextgenrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_gen_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.5/site-packages/textgenrnn/textgenrnn.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textgenrnn.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I'm going to add so much randomness to my day.... (using the default model)\n",
    "my_day_story=\"Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen.\"\n",
    "temp=0.9\n",
    "textgen.generate(prefix=my_day_story, temperature=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# That was too random... I will lower the temperature.\n",
    "temp=0.7\n",
    "textgen.generate(prefix=my_day_story, temperature=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNNs to generate fake cryptocurrency data\n",
    "\n",
    "I used the [textgenrnn](https://github.com/minimaxir/textgenrnn) RNN machine learning module to generate fake/speculative cryptocurrency data from current cryptocurrency data (it’s this how it’s done anyhow ;-)).\n",
    "\n",
    "The output is in the form of __symbol, name, $USD price__\n",
    "\n",
    "Some favorites...\n",
    "\n",
    "```\n",
    "BOTC Botcoin 0.00021108\n",
    "CRT \"Credit Token\" 0.000653993\n",
    "BOT BootCoin 0.000129398\n",
    "SPC Spacecoin 0.00012733\n",
    "\n",
    "XCC \"Cash Coin\" 0.0\n",
    "XCC \"Chin Coin\" 0.000139998\n",
    "GET2 \"Gene Token\" 0.000113792\n",
    "BET Betters 0.0\n",
    "DARE Darto 0.0\n",
    "STT3 \"Start Coin\" 0.0\n",
    "PCC2 \"Place Coin\" 0.000148341\n",
    "XBC \"Bitcoin Blockchain\" 0.000523123\n",
    "PARE Paris 0.0\n",
    "BARE BitcoinA 0.001190911\n",
    "```\n",
    "\n",
    "And then by increasing the ‘temperature’ (adding more  ‘entropy’) they get even better…\n",
    "\n",
    "```\n",
    "CTA1 \"Currenity Token\" 0.0\n",
    "KROB Krep 0.0\n",
    "FXC FuxxCoin 0.0\n",
    "EMC \"Decert Money Coin\" 0.001999197\n",
    "BROTC Brostribs 0.0\n",
    "XBAC2 \"Blockchain Adiamend Coin\" 0.001187232\n",
    "DECO Delcoin 0.0\n",
    "HCC HickCoin 0.0\n",
    "WOP Wo 0.695537238\n",
    "MAPL \"Marpa Chain\" 5.189e-06\n",
    "BITM Bitmi 0.0\n",
    "BRIX Biocoin 0.000149343\n",
    "SHHT \"SHT Token\" 0.0\n",
    "KM2 Kikera 0.0\n",
    "SECN Secus 0.0\n",
    "NSC Nucoin 0.0\n",
    "PCOP PoperCoin 0.000465216\n",
    "```\n",
    "\n",
    "## Method / Data / How\n",
    "The data source is all cryptocurrencies listed by coindex (including those no longer trading).\n",
    "The data was ingested from coindex as JSON and then transformed into a csv/txt file that works with the minimaxir/textgenrnn module.\n",
    "[Data source](https://coincodex.com/apps/coincodex/cache/all_coins_packed.json?t=26199381&coincodex.com)\n",
    "\n",
    "All the code for this data transformation is in [github](https://github.com/aberke/city-visions/blob/master/week-2) at `./crypto_data_script.ipynb`\n",
    "\n",
    "Output datafile: `./data/cryptocurrencies_data.txt`\n",
    "\n",
    "*Note: Many of the cryptocurrencies used as training input are no longer trading, in which case their $USD price was set to 0.0.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,318 texts collected.\n",
      "Training new model w/ 2-layer, 128-cell Bidirectional LSTMs\n",
      "Training on 150,441 character sequences.\n",
      "Epoch 1/3\n",
      "1175/1175 [==============================] - 26s 22ms/step - loss: 2.1343\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "TRC2 TronCoin 0.0\n",
      "\n",
      "LTC Lotthron 0.000999989\n",
      "\n",
      "SPC SppperCoin 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "HNG \"Mond Coin\" 0.001072389\n",
      "\n",
      "LLT Lomernes 0.0\n",
      "\n",
      "SBC StareCoin 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "FHU Frium 0.000988283\n",
      "\n",
      "ORV2 \"Orvine Prad\" 0.005528285\n",
      "\n",
      "SBX2 Smenf 0.0\n",
      "\n",
      "Epoch 2/3\n",
      "1175/1175 [==============================] - 25s 21ms/step - loss: 1.8024\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "CONT CONTOKEN 0.0\n",
      "\n",
      "CORE CORON 0.0\n",
      "\n",
      "ENT Enter 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "BUNT Buntor 0.0\n",
      "\n",
      "GRA GaraCoin 0.000367625\n",
      "\n",
      "VONT VOANTOKONTOKEN 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "WAWBTA \"ARBTCU Token\" 0.165626623\n",
      "\n",
      "ACA2 Alcohia 0.000282005\n",
      "\n",
      "BBORCONY BORARAR 0.4\n",
      "\n",
      "Epoch 3/3\n",
      "1175/1175 [==============================] - 25s 21ms/step - loss: 1.6728\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "CRO Coinoma 0.001222939\n",
      "\n",
      "BTC Bitcoin 0.0\n",
      "\n",
      "BET2 \"Bitcoin Token\" 0.00013473\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "BOL Bule 0.000106375\n",
      "\n",
      "BEC Betcoin 0.000885678\n",
      "\n",
      "FURE Furex 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "BCK Brickecoin 0.0\n",
      "\n",
      "WINT WAN 0.0\n",
      "\n",
      "IGE \"Indecure Monery Orency\" 0.00031321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_cryptos_gen = textgenrnn(name=\"fake_cryptos\")\n",
    "fake_cryptos_gen.reset()\n",
    "fake_cryptos_gen.train_from_file(\n",
    "    './data/cryptocurrencies_data.txt',\n",
    "    new_model=True,\n",
    "    rnn_bidirectional=True,\n",
    "    dim_embeddings=300,\n",
    "    num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:00<00:04,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT BootCoin 0.000129398\n",
      "\n",
      "STRT Strea 0.0\n",
      "\n",
      "CRT \"Credit Token\" 0.000653993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:04,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORO TronCoin 0.001070109\n",
      "\n",
      "BOTC Botcoin 0.00021108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:00<00:04,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPA Payancoin 0.000219937\n",
      "\n",
      "SPC Spacecoin 0.00012733\n",
      "\n",
      "XCC \"Cash Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:00<00:04,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONA Mononio 0.0\n",
      "\n",
      "XCC \"Chin Coin\" 0.000139998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:01<00:03, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC2 \"Bitcoin Token\" 0.001148301\n",
      "\n",
      "VERE Verio 0.0\n",
      "\n",
      "CNT CoinTraden 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:03, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOON \"JOO Coin\" 0.000197318\n",
      "\n",
      "ACOIN Acine 0.0\n",
      "\n",
      "RED Redio 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:02, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORC Corencoin 0.000329609\n",
      "\n",
      "MENT Mentrenity 0.0\n",
      "\n",
      "SHT \"Sher Token\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:01<00:02, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET2 \"Gene Token\" 0.000113792\n",
      "\n",
      "PRC2 Priplic 0.000171769\n",
      "\n",
      "KOND Konda 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:02<00:02, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRO TronLite 0.0\n",
      "\n",
      "BITE BITTY 0.0\n",
      "\n",
      "HAT \"Hash Token\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:02<00:01, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTR Bitcoin 0.000109439\n",
      "\n",
      "BET Betters 0.0\n",
      "\n",
      "TRT1 TRRES 0.0\n",
      "\n",
      "STC Starto 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAT Batcoin 0.000186939\n",
      "\n",
      "CORE CoinCoin 0.000197231\n",
      "\n",
      "EXC Excoin 0.001291339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:03<00:01, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRT2 Tronerto 0.0\n",
      "\n",
      "DARE Darto 0.0\n",
      "\n",
      "STT3 \"Start Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:03<00:01, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STC Starter 0.0\n",
      "\n",
      "MOD \"Moder Coin\" 0.000103397\n",
      "\n",
      "DTC Dectream 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:03<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC2 \"Place Coin\" 0.000148341\n",
      "\n",
      "VET Vetalio 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:03<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBC \"Bitcoin Blockchain\" 0.000523123\n",
      "\n",
      "CORX Corex 0.0\n",
      "\n",
      "STC Stace 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [00:04<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCT2 \"Bitcoin Token\" 0.0\n",
      "\n",
      "UPT UPToken 0.0\n",
      "\n",
      "BETB Bitcoin 0.0\n",
      "\n",
      "ALT Allation 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCA Anacoin 0.000113473\n",
      "\n",
      "PARE Paris 0.0\n",
      "\n",
      "BARE BitcoinA 0.001190911\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N=50\n",
    "fake_cryptos_gen.generate(\n",
    "    n=N,\n",
    "    return_as_list=False,\n",
    "    temperature=[0.5, 0.3, 0.2],\n",
    "    max_gen_length=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:05,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTA1 \"Currenity Token\" 0.0\n",
      "\n",
      "KROB Krep 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:00<00:04,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBAC2 \"Blockchain Adiamend Coin\" 0.001187232\n",
      "\n",
      "BIC Bitcoin 0.000153249\n",
      "\n",
      "FXC FuxxCoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:00<00:04, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATH Author 0.0\n",
      "\n",
      "VEB Deceneurbit 7.613e-06\n",
      "\n",
      "SUP Spoper 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:00<00:04,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMC \"Decert Money Coin\" 0.001999197\n",
      "\n",
      "BROTC Brostribs 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:01<00:03,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2OAS \"Soacoin Sure\" 0.130983886\n",
      "\n",
      "SELC Selus 0.003067594\n",
      "\n",
      "BHP Bitpocoin 0.000109744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:03, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERA ERP 0.000160913\n",
      "\n",
      "LOLI Luocoin 0.000160972\n",
      "\n",
      "GOA GAGO 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:02, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECO Delcoin 0.0\n",
      "\n",
      "HCC HickCoin 0.0\n",
      "\n",
      "WOP Wo 0.695537238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:01<00:02, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIS Bitcoin 0.00094139\n",
      "\n",
      "KDO \"KODP Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:02<00:02,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATV \"Atreamond Token\" 0.00138699\n",
      "\n",
      "SDT \"Steper Disiance Coin\" 0.000299439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:02<00:02, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAR Elargar 0.0\n",
      "\n",
      "TTX Tartexuri 0.0\n",
      "\n",
      "DUARD DRORP 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:02<00:01, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRN Fraxi 0.000283347\n",
      "\n",
      "BUT2 Buste 0.000081548\n",
      "\n",
      "NENA Nexy 0.011165955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTA Blockports 0.000284311\n",
      "\n",
      "EBL Ecrep 0.030643852\n",
      "\n",
      "BBC2 Bitcoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:03<00:01, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART2 \"Car Trader Coin\" 0.0\n",
      "\n",
      "XME \"Hextine Coin\" 0.007289132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:03<00:01, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPL \"Marpa Chain\" 5.189e-06\n",
      "\n",
      "BITM Bitmi 0.0\n",
      "\n",
      "BRIX Biocoin 0.000149343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:03<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHHT \"SHT Token\" 0.0\n",
      "\n",
      "KM2 Kikera 0.0\n",
      "\n",
      "SECN Secus 0.0\n",
      "\n",
      "NSC Nucoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:03<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Redvolo 0.0\n",
      "\n",
      "XSD \"Spold Findation\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [00:04<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXLC EmeroCoin 0.000253889\n",
      "\n",
      "RCTC Recoin 0.0\n",
      "\n",
      "WONO WoonCoin 0.000091916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:04<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS STRT 0.000383109\n",
      "\n",
      "FR2 FSG 0.0\n",
      "\n",
      "PCOP PoperCoin 0.000465216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOT Eather 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# And then trying with more 'entropy' or a higher 'temperature'\n",
    "fake_cryptos_gen.generate(\n",
    "    n=N,\n",
    "    return_as_list=False,\n",
    "    temperature=[0.8, 0.7],\n",
    "    max_gen_length=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are relative newcomers to the language processing world. They are an evolution of recurrent neural networks and activation layers. Using transformers has increased the capability of generating believable text by a whole lot, so much so that [ethical issues](https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harmful-programs-openai) have arised around release of models or restrictive use of them.\n",
    "\n",
    "Architecture wise, transformers are an encoder-decoder scheme that relies heavily on \"attention\" - a mechanism that allows every step to examine both past and future.\n",
    "\n",
    "<img src=\"http://lilianweng.github.io/lil-log/assets/images/transformer.png\" />\n",
    "\n",
    "One recent model from OpenAI is GPT-2, which is freely available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/59/d88fe8c58ffb66aca21d03c0e290cd68327cc133591130c674985e98a482/tensorflow-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 109.2MB 13kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/01/e3760e3210e1f67f871b270b7ac2c1c819794fe34726d6d8bc8322fa2a57/grpcio-1.24.3-cp27-cp27mu-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 640kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/95/d41cd87d147742ef72d5d1dc317318486e3fbffdadf24a60e70dedf01d56/google_pasta-0.1.7-py2-none-any.whl (55kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting wheel (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting enum34>=1.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/49/ffa7ab9c52ec56b535cffec3bc844254c073888e6d4aeee464671ac97480/protobuf-3.10.0-cp27-cp27mu-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 486kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 12.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 523kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting futures>=2.2.0; python_version < \"3.2\" (from grpcio>=1.8.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/9a/50fadfd53ec909e4399b67c74cc7f4e883488035cfcdb90b685758fa8b34/setuptools-41.4.0-py2.py3-none-any.whl (580kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 12.2MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: keras-applications, termcolor, wrapt, gast, absl-py\n",
      "  Running setup.py bdist_wheel for keras-applications ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "Successfully built keras-applications termcolor wrapt gast absl-py\n",
      "Installing collected packages: numpy, six, h5py, keras-applications, funcsigs, mock, futures, enum34, grpcio, termcolor, google-pasta, tensorflow-estimator, backports.weakref, wheel, astor, wrapt, gast, keras-preprocessing, setuptools, protobuf, absl-py, werkzeug, markdown, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.0 backports.weakref-1.0.post1 enum34-1.1.6 funcsigs-1.0.2 futures-3.3.0 gast-0.3.2 google-pasta-0.1.7 grpcio-1.24.3 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.16.5 protobuf-3.10.0 setuptools-41.4.0 six-1.12.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 11104, done.\u001b[K\n",
      "remote: Total 11104 (delta 0), reused 0 (delta 0), pack-reused 11104\u001b[K\n",
      "Receiving objects: 100% (11104/11104), 5.52 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (8099/8099), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: conda: not found\n"
     ]
    }
   ],
   "source": [
    "!conda update anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/e9/90851b92b606c4fdd13fa971dd0d47e500736ee9e56fd94570341f08700c/torch-1.3.0-cp27-cp27mu-manylinux1_x86_64.whl (773.1MB)\n",
      "\u001b[K    99% |████████████████████████████████| 773.1MB 62.3MB/s eta 0:00:01\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 353, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 749, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 620, in _prepare_file\n",
      "    session=self.session, hashes=hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 821, in unpack_url\n",
      "    hashes=hashes\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 659, in unpack_http_url\n",
      "    hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 882, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 603, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/utils/hashes.py\", line 46, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 571, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/utils/ui.py\", line 139, in iter\n",
      "    for x in it:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 560, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/usr/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 432, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 380, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 63, in read\n",
      "    self._close()\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 50, in _close\n",
      "    self.__callback(self.__buf.getvalue())\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/controller.py\", line 275, in cache_response\n",
      "    self.serializer.dumps(request, response, body=body),\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/serialize.py\", line 87, in dumps\n",
      "    ).encode(\"utf8\"),\n",
      "MemoryError\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8103, 0.4069, 0.4805],\n",
      "        [0.5968, 0.7617, 0.2540],\n",
      "        [0.7770, 0.4677, 0.5738],\n",
      "        [0.8670, 0.3628, 0.8548],\n",
      "        [0.2817, 0.1889, 0.5832]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your user gives you lemons you generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./transformers/examples/run_generation.py\", line 25, in <module>\n",
      "    import torch\n",
      "ImportError: No module named torch\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" #\\\n",
    "#     --prompt=\"When life gives you lemons\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --prompt=\"Harry witnessed Professor McGonagall walking right past Peeves who \\\n",
    "was determinedly loosening a crystal chandelier and could have sworn he heard her \\\n",
    "tell the poltergeist out of the corner of her mouth, 'It unscrews the other way.’\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's see how it does with Williams' \"This is just to say\" (https://poets.org/poem/just-say) poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"I have eaten the plums \\\n",
    "that were in the icebox \\\n",
    "and which you were probably \\\n",
    "saving for breakfast\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this BTW is one of my favorite poems ever. So sweet and so plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefix inputs\n",
    "* Try different temperatures with the `--temperature` argument.\n",
    "* Advanced: Try a different model than GPT-2.\n",
    "\n",
    "On the help section for the generation script you can find all the models:\n",
    "```\n",
    "--model_name_or_path MODEL_NAME_OR_PATH\n",
    "                        Path to pre-trained model or shortcut name selected in\n",
    "                        the list: gpt2, gpt2-medium, gpt2-large, distilgpt2,\n",
    "                        openai-gpt, xlnet-base-cased, xlnet-large-cased,\n",
    "                        transfo-xl-wt103, xlm-mlm-en-2048, xlm-mlm-ende-1024,\n",
    "                        xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-\n",
    "                        xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024,\n",
    "                        xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280,\n",
    "                        ctrl\n",
    "```\n",
    "The `ctrl` model is very recent work (from SalesForce research), just from a couple of weeks ago, it's supposed to be really awesome at controling the output text. Be warned - the model is a **6Gb download**! It might be worth it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot\n",
    "\n",
    "[Chatbots](https://en.wikipedia.org/wiki/Chatbot) are conversational AI agents that can respond to text input. It's still ways away from a convincing conversation in general open-ended scenarios, but in certain applications chatbots are a big success, e.g. in the public services industry's online portals.\n",
    "\n",
    "`huggingface` again have released their pretrained models for chatbots based on transformers just a few months ago: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#79c5\n",
    "\n",
    "You can also use their online demo: https://convai.huggingface.co/persona/my-only-friend-is-a-dog-i-work-at-a-newspaper-my-father-used-to-be-a-butcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 353, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 749, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 620, in _prepare_file\n",
      "    session=self.session, hashes=hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 821, in unpack_url\n",
      "    hashes=hashes\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 659, in unpack_http_url\n",
      "    hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 882, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 603, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/utils/hashes.py\", line 46, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 571, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 560, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/usr/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 432, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 380, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 63, in read\n",
      "    self._close()\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 50, in _close\n",
      "    self.__callback(self.__buf.getvalue())\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/controller.py\", line 275, in cache_response\n",
      "    self.serializer.dumps(request, response, body=body),\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/serialize.py\", line 87, in dumps\n",
      "    ).encode(\"utf8\"),\n",
      "MemoryError\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transfer-learning-conv-ai'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 96 (delta 0), reused 1 (delta 0), pack-reused 89\u001b[K\n",
      "Unpacking objects: 100% (96/96), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transfer-learning-conv-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_proc():\n",
    "    proc = subprocess.Popen([sys.executable, \n",
    "                             os.getcwd()+'/transfer-learning-conv-ai/interact.py'\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stdin=subprocess.PIPE,\n",
    "                            stderr=subprocess.DEVNULL)\n",
    "    pout = proc.stdout\n",
    "    pin = proc.stdin\n",
    "    \n",
    "    return proc,pout,pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc, cb1_pout, cb1_pin = chatbot_proc(); # create a chatbot process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_pin.write(b\"--temperature=1.1\\n\"), cb1_pin.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> hi how are you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i'm doing well just listening to some music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"i'm doing mighty fine! and how are you?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i am listening to a lot of pop music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"no way! i'm also listening to music. what music are you listening to?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also quite funny to get it to talk to itself - it never get tired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_output = b\"i am listening to a lot of pop music\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: yeah, i know what you mean.\n",
      "A: what do you do for a living?\n",
      "B: i am a mechanic.\n",
      "A: i am a pilot.\n",
      "B: what do you do for work?\n",
      "A: i fix planes.\n",
      "B: what kind of planes do you have?\n",
      "A: do you have any hobbies?\n",
      "B: i like to listen to music.\n",
      "A: what kind of music do you like?\n"
     ]
    }
   ],
   "source": [
    "partyA = True\n",
    "for _ in range(10):\n",
    "    partyA = not partyA\n",
    "    cb1_pin.write(cb1_output), cb1_pin.flush();\n",
    "    cb1_output = cb1_pout.readline()[4:]\n",
    "    print(\"%s: %s\" % ('A' if partyA else 'B',\n",
    "          cb1_output[:-1].decode(sys.stdout.encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc.kill() # kill the chatbot process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try some different inputs\n",
    "* Advanced: Spin up another chatbot and have them talk to one another (by feeding the outputs across)\n",
    "* Advanced: Use a different underlying model than GPT-2 for your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's a wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
