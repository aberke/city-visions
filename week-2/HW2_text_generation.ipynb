{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Generative Models\n",
    "\n",
    "In this assignment we will see some generative models for text: CharRNN, Transformers and Chatbots. Training text models is very time consuming, and uses a ton of data. The really good models also tend to be very large, so we will stick to pretrained models. Those can still be excellent to generate totally new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Embeddings are numeric representations for non-numeric data. In our case we look for embeddings for words. A simple kind of embedding is One-Hot Encoding, where we put a `1` in a vector of all `0`s at the index of the word in the vocabulary.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "But that can be very wasteful and also doesn't encode any relationship between the words.\n",
    "\n",
    "To learn semantic relationship a few unsupervised algorithms were proposed. In class we've discussed Continuous Bag of Words and Skip-Gram. Essentially these will mask out part of a sentence and ask the model to predict the missing part. This way the model learns about the context words are used in sentences as well as relationships.\n",
    "\n",
    "Embedding for a word is a vector of numbers:\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" width=\"50%\" />\n",
    "\n",
    "Luckily many of the world leaders in natural language processing have pretrained word embeddings learned on huge corpora, so we don't have to do it ourselves.\n",
    "\n",
    "Allison Parrish of NYU showed some very interesting uses for word embeddings for poetry generation: https://www.youtube.com/watch?v=L3D0JEA1Jdc breeze through this StrangeLoop talk for inspiration. I encourage you to try these methods towards you own generative work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chakin` is a helper tool for downloading pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q chakin progressbar2 textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chakin\n",
    "import progressbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search('English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download GLoVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:06:27   2.1 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./glove.6B.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chakin.download(number=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need one file (the smallest dimension one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the embedding values for each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n"
     ]
    }
   ],
   "source": [
    "!head -5 glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load them into memory and organize a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_lines = open('glove.6B.50d.txt','rt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100000 of 100000) |################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "w2v_emb_dict = dict()\n",
    "pbar = progressbar.ProgressBar(max_value=100000)\n",
    "for i,l in enumerate(w2vec_lines[1:100000]):\n",
    "    w,emb = l.split(' ', 1)\n",
    "    w2v_emb_dict[w] = np.fromstring(emb, sep=' ')\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would be the first most commonly used tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authoritatively',\n",
       " '1,520',\n",
       " 'quinton',\n",
       " 'editorialized',\n",
       " 'beutel',\n",
       " 'gashes',\n",
       " 'pronounced',\n",
       " 'bettered',\n",
       " 'jagdish',\n",
       " 'eglin']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w2v_emb_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies and Similarities\n",
    "\n",
    "Embeddings carry semantic information in their numeric encoding. Exploring this semantic space can be fun, for example looking for similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is measuring the angle between vectors. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2432/1*Acs3Kbrrrb4d3fqMlGhMcQ.png\"/>\n",
    "\n",
    "Our embeddings are normalized vectors so looking at the angle between two vectors reveals how far away they are from one another in the high-dimensional embdding space:\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "w2v_emb_dict_keys = list(w2v_emb_dict.keys())\n",
    "w2v_emb_dict_values = np.array(list(w2v_emb_dict.values()))\n",
    "\n",
    "def find_nearest(w):\n",
    "    return w2v_emb_dict_keys[cosine_similarity(w2v_emb_dict[w].reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-2]]\n",
    "def find_nearest_top_k(v, k=5):\n",
    "    return [w2v_emb_dict_keys[w] for w in cosine_similarity(v.reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-k:].tolist()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at closest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaching'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider \"**word analogies**\", e.g. completing the sentence: \"Paris is to France like Rome is to ___\" \\(Italy\\)\n",
    "\n",
    "To explain this geometrically:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2632/1*EOVxNmHkrsPQ7Q44N0OiQg.png\" width=\"60%\" />\n",
    "\n",
    "The offset vector between \"paris\" and \"france\" is the \"Captial of\" vector, and when we apply it to \"rome\" we expect to get \"italy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italy', 'spain', 'rome', 'portugal', 'france']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['france'] - w2v_emb_dict['paris'] + w2v_emb_dict['rome'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'daughter', 'prince', 'throne']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['king'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following analogies:\n",
    "1. sushi-rice is like pizza-___\n",
    "2. sushi-rice is like steak-___\n",
    "3. shirt-clothing is like phone-___\n",
    "4. shirt-clothing is like bowl-___\n",
    "5. book-reading is like TV-___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. sushi-rice is like pizza-___\n",
      "['pizza', 'sushi', 'fast-food', 'diner', 'nachos']\n"
     ]
    }
   ],
   "source": [
    "print('1. sushi-rice is like pizza-___')\n",
    "print(find_nearest_top_k(w2v_emb_dict['sushi'] - w2v_emb_dict['rice'] + w2v_emb_dict['pizza'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['steak', 'sushi', 'cheeseburger', 'steaks', 'meatball']\n"
     ]
    }
   ],
   "source": [
    "# 2. sushi-rice is like steak-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['sushi'] - w2v_emb_dict['rice'] + w2v_emb_dict['steak'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza', 'sandwich', 'hat', 'sandwiches', 'pie']\n"
     ]
    }
   ],
   "source": [
    "# 3. shirt-clothing is like phone-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['shirt'] - w2v_emb_dict['clothing'] + w2v_emb_dict['pizza'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bowl', 'crimson', 'afc', 'gator', 'super']\n"
     ]
    }
   ],
   "source": [
    "# 4. shirt-clothing is like bowl-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['shirt'] - w2v_emb_dict['clothing'] + w2v_emb_dict['bowl'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'television', 'movie', 'hbo', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# 5. book-reading is like TV-___\n",
    "print(find_nearest_top_k(w2v_emb_dict['book'] - w2v_emb_dict['reading'] + w2v_emb_dict['tv'], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find analogies that don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing around to find analogies that do not work…\n",
    "\n",
    "Breakfast-morning is like dinner-____\n",
    "\n",
    "Breakfast-morning is like lunch-_____\n",
    "\n",
    "Pie-dessert is like broccoli-____\n",
    "\n",
    "cake-dessert is like spinach-____\n",
    "\n",
    "Ketchup-burger is like syrup-_____\n",
    "\n",
    "Hummus-carrots is like mustard-____\n",
    "\n",
    "Art-paint is like literature-_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dinners', 'breakfast', 'dinner', 'buffet', 'breakfasts']\n"
     ]
    }
   ],
   "source": [
    "# Breakfast-morning is like dinner-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['breakfast'] - w2v_emb_dict['morning'] + w2v_emb_dict['dinner'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breakfast', 'buffet', 'breakfasts', 'dinners', 'lunch']\n"
     ]
    }
   ],
   "source": [
    "# Breakfast-morning is like lunch-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['breakfast'] - w2v_emb_dict['morning'] + w2v_emb_dict['lunch'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['broccoli', 'cauliflower', 'zucchini', 'sprouts', 'celery']\n"
     ]
    }
   ],
   "source": [
    "# Pie-dessert is like broccoli-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['pie'] - w2v_emb_dict['dessert'] + w2v_emb_dict['broccoli'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spinach', 'lettuce', 'potatoes', 'peeled', 'carrots']\n"
     ]
    }
   ],
   "source": [
    "# Icecream-dessert is like spinach-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['cake'] - w2v_emb_dict['dessert'] + w2v_emb_dict['spinach'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syrup', 'vinegar', 'molasses', 'juice', 'vanilla']\n"
     ]
    }
   ],
   "source": [
    "# Ketchup-burger is like syrup-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['ketchup'] - w2v_emb_dict['burger'] + w2v_emb_dict['syrup'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hummus', 'naim', 'malak', 'ales', 'kassem']\n"
     ]
    }
   ],
   "source": [
    "# Hummus-carrots is like mustard-____\n",
    "print(find_nearest_top_k(w2v_emb_dict['hummus'] - w2v_emb_dict['carrots'] + w2v_emb_dict['mustard'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['literature', 'literary', 'poetry', 'scholar', 'contemporary']\n"
     ]
    }
   ],
   "source": [
    "# Art-paint is like literature-_____\n",
    "print(find_nearest_top_k(w2v_emb_dict['art'] - w2v_emb_dict['paint'] + w2v_emb_dict['literature'], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN\n",
    "\n",
    "CharRNN is a simple recurrent neural network architecture that works on the character level (not words). It's surprisingly powerful at generating text. These were popularized by [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"50%\"/>\n",
    "\n",
    "The `textgenrnn` package is a convnient way to train and generate with CharRNNs. Here we're using its built in model. They have multiple models [published](https://github.com/minimaxir/textgenrnn/tree/master/weights) trained on different corpora.\n",
    "\n",
    "People created some very cool projects with it: https://github.com/minimaxir/textgenrnn#projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1025 18:29:09.660571 139840947357440 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key confirmed for a random burger to be on the floor?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supply a prefix to prime the model with text to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When life gives you lemons as a month of the company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"When life gives you lemons \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also let the model try different \"temperatures\". The \"temperature\" controls the level of random choice when picking the next character, instead of always the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "[Specific] Can someone please remove the story of the streets and the most support is the best computer and why are the posts of the state of the best community to the post in the top of the state of the state of the same time to a super of the state of the state of the state of the starter of the \n",
      "\n",
      "[Specific] Can someone please remove the story of the programming in the most state of the state of the most planet in the state of the season in the back of the same things to get a girl to the strange with a huge character and become still going to be a man who has a stranger to the programming t\n",
      "\n",
      "The subreddit of the state of the state of the discovery of the same state of the same starting to the state of the first time that we say they are a big defender of the story of the state of the state of the state of the state of the story of the streets of the state of the same time in the first \n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "[Humor] I just learned to the best part of the best room of the best fun and I am there as he for the best for me for you.\n",
      "\n",
      "A study got me a base my Facebook that looks like a man who seemingly appreciate it.\"\n",
      "\n",
      "I want to complete the conversation of the team to the most game of the world is simple with the top of the basic crime is back to standard in the Anchor of the Draft Save and Morty - an Advice of the 'Senators' Christmas that can remove the good time in a time in the World friend who says he was s\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Thank you Morty, PsBattle: orteuj makes me when they saw \"The Get Hard\"\n",
      "\n",
      "I work for which Reddit with your Monu, I've been jacked out the Repaire Book Caster [F]\n",
      "\n",
      "Bigs spia 'man's \"scared after learning cards\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefixes and temperatures. (examine the `.generate()` function, by running a cell with `textgenrnn.generate?`)\n",
    "* Try a different pretrained model from `textgenrnn`\n",
    "* Advanced: train your own model! `textgenrnn` provide a **very** simple mechanism to do so: https://github.com/minimaxir/textgenrnn#examples, you just need to supply a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtextgenrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_gen_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.5/site-packages/textgenrnn/textgenrnn.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textgenrnn.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I'm going to add so much randomness to my day.... (using the default model)\n",
    "my_day_story=\"Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen.\"\n",
    "temp=0.9\n",
    "textgen.generate(prefix=my_day_story, temperature=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I had a meeting at 11am and thought I would do work beforehand but then I ran into my thesis reader in the kitchen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# That was too random... I will lower the temperature.\n",
    "temp=0.7\n",
    "textgen.generate(prefix=my_day_story, temperature=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNNs to generate fake cryptocurrency data\n",
    "\n",
    "I used the [textgenrnn](https://github.com/minimaxir/textgenrnn) RNN machine learning module to generate fake/speculative cryptocurrency data from current cryptocurrency data (it’s this how it’s done anyhow ;-)).\n",
    "\n",
    "The output is in the form of __symbol, name, $USD price__\n",
    "\n",
    "Some favorites...\n",
    "\n",
    "```\n",
    "BOTC Botcoin 0.00021108\n",
    "CRT \"Credit Token\" 0.000653993\n",
    "BOT BootCoin 0.000129398\n",
    "SPC Spacecoin 0.00012733\n",
    "\n",
    "XCC \"Cash Coin\" 0.0\n",
    "XCC \"Chin Coin\" 0.000139998\n",
    "GET2 \"Gene Token\" 0.000113792\n",
    "BET Betters 0.0\n",
    "DARE Darto 0.0\n",
    "STT3 \"Start Coin\" 0.0\n",
    "PCC2 \"Place Coin\" 0.000148341\n",
    "XBC \"Bitcoin Blockchain\" 0.000523123\n",
    "PARE Paris 0.0\n",
    "BARE BitcoinA 0.001190911\n",
    "```\n",
    "\n",
    "And then by increasing the ‘temperature’ (adding more  ‘entropy’) they get even better…\n",
    "\n",
    "```\n",
    "CTA1 \"Currenity Token\" 0.0\n",
    "KROB Krep 0.0\n",
    "FXC FuxxCoin 0.0\n",
    "EMC \"Decert Money Coin\" 0.001999197\n",
    "BROTC Brostribs 0.0\n",
    "XBAC2 \"Blockchain Adiamend Coin\" 0.001187232\n",
    "DECO Delcoin 0.0\n",
    "HCC HickCoin 0.0\n",
    "WOP Wo 0.695537238\n",
    "MAPL \"Marpa Chain\" 5.189e-06\n",
    "BITM Bitmi 0.0\n",
    "BRIX Biocoin 0.000149343\n",
    "SHHT \"SHT Token\" 0.0\n",
    "KM2 Kikera 0.0\n",
    "SECN Secus 0.0\n",
    "NSC Nucoin 0.0\n",
    "PCOP PoperCoin 0.000465216\n",
    "```\n",
    "\n",
    "## Method / Data / How\n",
    "The data source is all cryptocurrencies listed by coindex (including those no longer trading).\n",
    "The data was ingested from coindex as JSON and then transformed into a csv/txt file that works with the minimaxir/textgenrnn module.\n",
    "[Data source](https://coincodex.com/apps/coincodex/cache/all_coins_packed.json?t=26199381&coincodex.com)\n",
    "\n",
    "All the code for this data transformation is in [github](https://github.com/aberke/city-visions/blob/master/week-2) at `./crypto_data_script.ipynb`\n",
    "\n",
    "Output datafile: `./data/cryptocurrencies_data.txt`\n",
    "\n",
    "*Note: Many of the cryptocurrencies used as training input are no longer trading, in which case their $USD price was set to 0.0.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,318 texts collected.\n",
      "Training new model w/ 2-layer, 128-cell Bidirectional LSTMs\n",
      "Training on 150,441 character sequences.\n",
      "Epoch 1/3\n",
      "1175/1175 [==============================] - 26s 22ms/step - loss: 2.1343\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "TRC2 TronCoin 0.0\n",
      "\n",
      "LTC Lotthron 0.000999989\n",
      "\n",
      "SPC SppperCoin 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "HNG \"Mond Coin\" 0.001072389\n",
      "\n",
      "LLT Lomernes 0.0\n",
      "\n",
      "SBC StareCoin 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "FHU Frium 0.000988283\n",
      "\n",
      "ORV2 \"Orvine Prad\" 0.005528285\n",
      "\n",
      "SBX2 Smenf 0.0\n",
      "\n",
      "Epoch 2/3\n",
      "1175/1175 [==============================] - 25s 21ms/step - loss: 1.8024\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "CONT CONTOKEN 0.0\n",
      "\n",
      "CORE CORON 0.0\n",
      "\n",
      "ENT Enter 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "BUNT Buntor 0.0\n",
      "\n",
      "GRA GaraCoin 0.000367625\n",
      "\n",
      "VONT VOANTOKONTOKEN 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "WAWBTA \"ARBTCU Token\" 0.165626623\n",
      "\n",
      "ACA2 Alcohia 0.000282005\n",
      "\n",
      "BBORCONY BORARAR 0.4\n",
      "\n",
      "Epoch 3/3\n",
      "1175/1175 [==============================] - 25s 21ms/step - loss: 1.6728\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "CRO Coinoma 0.001222939\n",
      "\n",
      "BTC Bitcoin 0.0\n",
      "\n",
      "BET2 \"Bitcoin Token\" 0.00013473\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "BOL Bule 0.000106375\n",
      "\n",
      "BEC Betcoin 0.000885678\n",
      "\n",
      "FURE Furex 0.0\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "BCK Brickecoin 0.0\n",
      "\n",
      "WINT WAN 0.0\n",
      "\n",
      "IGE \"Indecure Monery Orency\" 0.00031321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_cryptos_gen = textgenrnn(name=\"fake_cryptos\")\n",
    "fake_cryptos_gen.reset()\n",
    "fake_cryptos_gen.train_from_file(\n",
    "    './data/cryptocurrencies_data.txt',\n",
    "    new_model=True,\n",
    "    rnn_bidirectional=True,\n",
    "    dim_embeddings=300,\n",
    "    num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:00<00:04,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT BootCoin 0.000129398\n",
      "\n",
      "STRT Strea 0.0\n",
      "\n",
      "CRT \"Credit Token\" 0.000653993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:04,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORO TronCoin 0.001070109\n",
      "\n",
      "BOTC Botcoin 0.00021108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:00<00:04,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPA Payancoin 0.000219937\n",
      "\n",
      "SPC Spacecoin 0.00012733\n",
      "\n",
      "XCC \"Cash Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:00<00:04,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONA Mononio 0.0\n",
      "\n",
      "XCC \"Chin Coin\" 0.000139998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:01<00:03, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC2 \"Bitcoin Token\" 0.001148301\n",
      "\n",
      "VERE Verio 0.0\n",
      "\n",
      "CNT CoinTraden 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:03, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOON \"JOO Coin\" 0.000197318\n",
      "\n",
      "ACOIN Acine 0.0\n",
      "\n",
      "RED Redio 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:02, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORC Corencoin 0.000329609\n",
      "\n",
      "MENT Mentrenity 0.0\n",
      "\n",
      "SHT \"Sher Token\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:01<00:02, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET2 \"Gene Token\" 0.000113792\n",
      "\n",
      "PRC2 Priplic 0.000171769\n",
      "\n",
      "KOND Konda 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:02<00:02, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRO TronLite 0.0\n",
      "\n",
      "BITE BITTY 0.0\n",
      "\n",
      "HAT \"Hash Token\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:02<00:01, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTR Bitcoin 0.000109439\n",
      "\n",
      "BET Betters 0.0\n",
      "\n",
      "TRT1 TRRES 0.0\n",
      "\n",
      "STC Starto 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAT Batcoin 0.000186939\n",
      "\n",
      "CORE CoinCoin 0.000197231\n",
      "\n",
      "EXC Excoin 0.001291339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:03<00:01, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRT2 Tronerto 0.0\n",
      "\n",
      "DARE Darto 0.0\n",
      "\n",
      "STT3 \"Start Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:03<00:01, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STC Starter 0.0\n",
      "\n",
      "MOD \"Moder Coin\" 0.000103397\n",
      "\n",
      "DTC Dectream 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:03<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC2 \"Place Coin\" 0.000148341\n",
      "\n",
      "VET Vetalio 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:03<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBC \"Bitcoin Blockchain\" 0.000523123\n",
      "\n",
      "CORX Corex 0.0\n",
      "\n",
      "STC Stace 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [00:04<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCT2 \"Bitcoin Token\" 0.0\n",
      "\n",
      "UPT UPToken 0.0\n",
      "\n",
      "BETB Bitcoin 0.0\n",
      "\n",
      "ALT Allation 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCA Anacoin 0.000113473\n",
      "\n",
      "PARE Paris 0.0\n",
      "\n",
      "BARE BitcoinA 0.001190911\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N=50\n",
    "fake_cryptos_gen.generate(\n",
    "    n=N,\n",
    "    return_as_list=False,\n",
    "    temperature=[0.5, 0.3, 0.2],\n",
    "    max_gen_length=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:05,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTA1 \"Currenity Token\" 0.0\n",
      "\n",
      "KROB Krep 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:00<00:04,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBAC2 \"Blockchain Adiamend Coin\" 0.001187232\n",
      "\n",
      "BIC Bitcoin 0.000153249\n",
      "\n",
      "FXC FuxxCoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:00<00:04, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATH Author 0.0\n",
      "\n",
      "VEB Deceneurbit 7.613e-06\n",
      "\n",
      "SUP Spoper 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:00<00:04,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMC \"Decert Money Coin\" 0.001999197\n",
      "\n",
      "BROTC Brostribs 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:01<00:03,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2OAS \"Soacoin Sure\" 0.130983886\n",
      "\n",
      "SELC Selus 0.003067594\n",
      "\n",
      "BHP Bitpocoin 0.000109744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:03, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERA ERP 0.000160913\n",
      "\n",
      "LOLI Luocoin 0.000160972\n",
      "\n",
      "GOA GAGO 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:02, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECO Delcoin 0.0\n",
      "\n",
      "HCC HickCoin 0.0\n",
      "\n",
      "WOP Wo 0.695537238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:01<00:02, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIS Bitcoin 0.00094139\n",
      "\n",
      "KDO \"KODP Coin\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:02<00:02,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATV \"Atreamond Token\" 0.00138699\n",
      "\n",
      "SDT \"Steper Disiance Coin\" 0.000299439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:02<00:02, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAR Elargar 0.0\n",
      "\n",
      "TTX Tartexuri 0.0\n",
      "\n",
      "DUARD DRORP 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:02<00:01, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRN Fraxi 0.000283347\n",
      "\n",
      "BUT2 Buste 0.000081548\n",
      "\n",
      "NENA Nexy 0.011165955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTA Blockports 0.000284311\n",
      "\n",
      "EBL Ecrep 0.030643852\n",
      "\n",
      "BBC2 Bitcoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:03<00:01, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART2 \"Car Trader Coin\" 0.0\n",
      "\n",
      "XME \"Hextine Coin\" 0.007289132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:03<00:01, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPL \"Marpa Chain\" 5.189e-06\n",
      "\n",
      "BITM Bitmi 0.0\n",
      "\n",
      "BRIX Biocoin 0.000149343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:03<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHHT \"SHT Token\" 0.0\n",
      "\n",
      "KM2 Kikera 0.0\n",
      "\n",
      "SECN Secus 0.0\n",
      "\n",
      "NSC Nucoin 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:03<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Redvolo 0.0\n",
      "\n",
      "XSD \"Spold Findation\" 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [00:04<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXLC EmeroCoin 0.000253889\n",
      "\n",
      "RCTC Recoin 0.0\n",
      "\n",
      "WONO WoonCoin 0.000091916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:04<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS STRT 0.000383109\n",
      "\n",
      "FR2 FSG 0.0\n",
      "\n",
      "PCOP PoperCoin 0.000465216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOT Eather 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# And then trying with more 'entropy' or a higher 'temperature'\n",
    "fake_cryptos_gen.generate(\n",
    "    n=N,\n",
    "    return_as_list=False,\n",
    "    temperature=[0.8, 0.7],\n",
    "    max_gen_length=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are relative newcomers to the language processing world. They are an evolution of recurrent neural networks and activation layers. Using transformers has increased the capability of generating believable text by a whole lot, so much so that [ethical issues](https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harmful-programs-openai) have arised around release of models or restrictive use of them.\n",
    "\n",
    "Architecture wise, transformers are an encoder-decoder scheme that relies heavily on \"attention\" - a mechanism that allows every step to examine both past and future.\n",
    "\n",
    "<img src=\"http://lilianweng.github.io/lil-log/assets/images/transformer.png\" />\n",
    "\n",
    "One recent model from OpenAI is GPT-2, which is freely available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Tensor(\"Sum:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Verify the install:\n",
    "import tensorflow as tf\n",
    "!python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your user gives you lemons you generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", it might not require much, but it can definitely have its benefits\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"When life gives you lemons\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\r\n",
      "\r\n",
      "And suddenly there  was a powerful tidal wave coming out of her ears.\r\n",
      "\r\n",
      "'Hey..'\r\n",
      "\r\n",
      "Professor McGonagall shook her head with a glance at Goyle,\r\n",
      "\r\n",
      "'Woof! Don't...I'm getting...shit! Shit!'\r\n",
      "\r\n",
      "Goyle was covered in flaming hair\r\n",
      "\r\n",
      "'Thank god I didn't say anything about it!'\r\n",
      "\r\n",
      "Now there was blood flowing from Professor McGonagall's face as she saw. Something yellow oozed out of her nose.\r\n",
      "\r\n",
      "Faint red vomit appeared from the spot.\r\n",
      "\r\n",
      "'Guzzle! Can you smell...!'\r\n",
      "\r\n",
      "Instantly other students quickly followed in dark circles around the teleportation weapon.\r\n",
      "\r\n",
      "Instantly darker objects appeared.\r\n",
      "\r\n",
      "One of the red objects looked like a shinier one or two scuttlecheeks.\r\n",
      "\r\n",
      "The red color drained from Goyle's face.\r\n",
      "\r\n",
      "Goyle let out a terror without smiling.\r\n",
      "\r\n",
      "But,\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --prompt=\"Harry witnessed Professor McGonagall walking right past Peeves who \\\n",
    "was determinedly loosening a crystal chandelier and could have sworn he heard her \\\n",
    "tell the poltergeist out of the corner of her mouth, 'It unscrews the other way.’\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's see how it does with Williams' \"This is just to say\" (https://poets.org/poem/just-say) poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and at about noon you  told me to lie back and lay on my stomach  and get a taste of the plums with my mouth  then your interest was  amazing  and you   prepared for the picnic, so it was\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"I have eaten the plums \\\n",
    "that were in the icebox \\\n",
    "and which you were probably \\\n",
    "saving for breakfast\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this BTW is one of my favorite poems ever. So sweet and so plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefix inputs\n",
    "* Try different temperatures with the `--temperature` argument.\n",
    "* Advanced: Try a different model than GPT-2.\n",
    "\n",
    "On the help section for the generation script you can find all the models:\n",
    "```\n",
    "--model_name_or_path MODEL_NAME_OR_PATH\n",
    "                        Path to pre-trained model or shortcut name selected in\n",
    "                        the list: gpt2, gpt2-medium, gpt2-large, distilgpt2,\n",
    "                        openai-gpt, xlnet-base-cased, xlnet-large-cased,\n",
    "                        transfo-xl-wt103, xlm-mlm-en-2048, xlm-mlm-ende-1024,\n",
    "                        xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-\n",
    "                        xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024,\n",
    "                        xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280,\n",
    "                        ctrl\n",
    "```\n",
    "The `ctrl` model is very recent work (from SalesForce research), just from a couple of weeks ago, it's supposed to be really awesome at controling the output text. Be warned - the model is a **6Gb download**! It might be worth it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to use the transformers models as alternative ways to generate more fake cryptos.\n",
    "First with gpt-2 and minimal ‘temperature’….\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " USDT ChainLink 0.015514094 CMC CMC 0.015514094 CMC 0.015514094 CMC 0.015514094 CMC 0.015514094 C\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --temperature=0 \\\n",
    "    --prompt=\"BTC Bitcoin 7661.3\\\n",
    "ETH Ethereum 167.42\\\n",
    "XRP Ripple 0.282570591\\\n",
    "UNFLD UnfoldU 34.34\\\n",
    "USDT Tether 1.000337543\\\n",
    "TRX TRON 0.015509159\\\n",
    "ADA Cardano 0.038299367\\\n",
    "LINK ChainLink 2.75\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wasn't quite right.  \n",
    "I want the machine to produce sequences of `symbole, name, USD price` So I reintroduced stop tokens.\n",
    "This worked a bit better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BTC BTC 0.0075\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --temperature=0 \\\n",
    "    --stop_token='|' \\\n",
    "    --prompt='BTC Bitcoin 7661.3|\\\n",
    "ETH Ethereum 167.42|\\\n",
    "XRP Ripple 0.282570591|\\\n",
    "UNFLD UnfoldU 34.34|\\\n",
    "USDT Tether 1.000337543|\\\n",
    "TRX TRON 0.015509159|\\\n",
    "ADA Cardano 0.038299367|\\\n",
    "LINK ChainLink 2.75|' 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay BTC already exists... clearly I need to increase the temperature (and maybe add more cryptos ;-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "10/27/2019 14:44:45 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/aberke/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "10/27/2019 14:44:45 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/aberke/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "10/27/2019 14:44:45 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/aberke/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "10/27/2019 14:44:45 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "10/27/2019 14:44:45 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /Users/aberke/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "10/27/2019 14:44:49 - INFO - __main__ -   Namespace(device=device(type='cpu'), length=100, model_name_or_path='gpt2', model_type='gpt2', n_gpu=0, no_cuda=False, padding_text='', prompt=\"ETH Ethereum 167.42| AUD2 'Aussie  Digital' 0.65982667| XRP Ripple 0.282570591| UNFLD UnfoldU 34.34| USDT Tether 1.000337543| BCH 'Bitcoin Cash' 222.35| GRAM 'Telegram Open Network' 1.807366053| LTC Litecoin 52.63| BNB 'Binance Coin' 17.78| FCT Factom 2.59| CVC Civic 0.037939729| BOTX botXcoin 0.015452991| ICN Iconomi 0.211692693| RHOC RChain 0.066656079| PAI2 'Project Pai' 0.01704574| QBIT Qubitica 30.48| BNK Bankera 0.000996052| RDD ReddCoin 0.000845153| MOF 'Molecular Future' 0.583074748| R Revain 0.048948372| AION Aion 0.066565099|\", repetition_penalty=1.0, seed=42, stop_token='|', temperature=0.5, top_k=0, top_p=0.9, xlm_lang='')\n",
      "100%|█████████████████████████████████████████| 100/100 [00:48<00:00,  1.75it/s]\n",
      " TBC TBC 0.066552961\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=100 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --temperature=0.5 \\\n",
    "    --stop_token=\"|\" \\\n",
    "    --prompt=\"ETH Ethereum 167.42|\\\n",
    "AUD2 'Aussie  Digital' 0.65982667|\\\n",
    "XRP Ripple 0.282570591|\\\n",
    "UNFLD UnfoldU 34.34|\\\n",
    "USDT Tether 1.000337543|\\\n",
    "BCH 'Bitcoin Cash' 222.35|\\\n",
    "GRAM 'Telegram Open Network' 1.807366053|\\\n",
    "LTC Litecoin 52.63|\\\n",
    "BNB 'Binance Coin' 17.78|\\\n",
    "FCT Factom 2.59|\\\n",
    "CVC Civic 0.037939729|\\\n",
    "BOTX botXcoin 0.015452991|\\\n",
    "ICN Iconomi 0.211692693|\\\n",
    "RHOC RChain 0.066656079|\\\n",
    "PAI2 'Project Pai' 0.01704574|\\\n",
    "QBIT Qubitica 30.48|\\\n",
    "BNK Bankera 0.000996052|\\\n",
    "RDD ReddCoin 0.000845153|\\\n",
    "MOF 'Molecular Future' 0.583074748|\\\n",
    "R Revain 0.048948372|\\\n",
    "AION Aion 0.066565099|\" #2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the temperature by a bit and mixing up the prefix subset of input coins...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SIC SIC Network 0.012328581\r\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=100 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --temperature=0.8 \\\n",
    "    --stop_token=\"|\" \\\n",
    "    --prompt=\"ICN Iconomi 0.211692693|\\\n",
    "RHOC RChain 0.066656079|\\\n",
    "PAI2 'Project Pai' 0.01704574|\\\n",
    "QBIT Qubitica 30.48|\\\n",
    "BNK Bankera 0.000996052|\\\n",
    "RDD ReddCoin 0.000845153|\\\n",
    "MOF 'Molecular Future' 0.583074748|\\\n",
    "R Revain 0.048948372|\\\n",
    "AION Aion 0.066565099|\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out the openai-gpt model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aberke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json not found in cache or force_download set to True, downloading to /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpajygfsux\n",
      "100%|███████████████████████████████| 815973/815973 [00:00<00:00, 4582443.03B/s]\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   copying /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpajygfsux to cache at /Users/aberke/.cache/torch/transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   creating metadata file for /Users/aberke/.cache/torch/transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   removing temp file /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpajygfsux\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt not found in cache or force_download set to True, downloading to /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpivgu2x6x\n",
      "100%|███████████████████████████████| 458495/458495 [00:00<00:00, 3788675.77B/s]\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   copying /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpivgu2x6x to cache at /Users/aberke/.cache/torch/transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   creating metadata file for /Users/aberke/.cache/torch/transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "10/27/2019 14:58:28 - INFO - transformers.file_utils -   removing temp file /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpivgu2x6x\n",
      "10/27/2019 14:58:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /Users/aberke/.cache/torch/transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "10/27/2019 14:58:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /Users/aberke/.cache/torch/transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "10/27/2019 14:58:28 - WARNING - transformers.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "10/27/2019 14:58:29 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json not found in cache or force_download set to True, downloading to /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpdv1yx4t1\n",
      "100%|███████████████████████████████████████| 273/273 [00:00<00:00, 90005.11B/s]\n",
      "10/27/2019 14:58:29 - INFO - transformers.file_utils -   copying /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpdv1yx4t1 to cache at /Users/aberke/.cache/torch/transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
      "10/27/2019 14:58:29 - INFO - transformers.file_utils -   creating metadata file for /Users/aberke/.cache/torch/transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
      "10/27/2019 14:58:29 - INFO - transformers.file_utils -   removing temp file /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpdv1yx4t1\n",
      "10/27/2019 14:58:29 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /Users/aberke/.cache/torch/transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
      "10/27/2019 14:58:29 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "10/27/2019 14:58:29 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin not found in cache or force_download set to True, downloading to /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpnfv16rgb\n",
      "100%|█████████████████████████| 478750579/478750579 [02:30<00:00, 3187170.33B/s]\n",
      "10/27/2019 15:00:59 - INFO - transformers.file_utils -   copying /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpnfv16rgb to cache at /Users/aberke/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
      "10/27/2019 15:01:00 - INFO - transformers.file_utils -   creating metadata file for /Users/aberke/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
      "10/27/2019 15:01:00 - INFO - transformers.file_utils -   removing temp file /var/folders/wt/5pyhsskj1hgdzv_4hb1gpkpm0000gn/T/tmpnfv16rgb\n",
      "10/27/2019 15:01:00 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /Users/aberke/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/27/2019 15:01:03 - INFO - __main__ -   Namespace(device=device(type='cpu'), length=100, model_name_or_path='openai-gpt', model_type='openai-gpt', n_gpu=0, no_cuda=False, padding_text='', prompt=\"GRAM 'Telegram Open Network' 1.807366053| LTC Litecoin 52.63| BNB 'Binance Coin' 17.78| FCT Factom 2.59| CVC Civic 0.037939729| BOTX botXcoin 0.015452991| ICN Iconomi 0.211692693| RHOC RChain 0.066656079| PAI2 'Project Pai' 0.01704574| QBIT Qubitica 30.48| BNK Bankera 0.000996052| RDD ReddCoin 0.000845153| MOF 'Molecular Future' 0.583074748| R Revain 0.048948372| AION Aion 0.066565099|\", repetition_penalty=1.0, seed=42, stop_token='|', temperature=0.5, top_k=0, top_p=0.9, xlm_lang='')\n",
      "100%|█████████████████████████████████████████| 100/100 [00:35<00:00,  2.29it/s]\n",
      "rdl. 9093982744. \n",
      " \" i don't know what to say, \" said the reporter. \n",
      " \" i don't either, \" said the reporter. \" but i think we should do something about this. \" \n",
      " \" what? \" \n",
      " \" i mean, i think we should do something about this. \" \n",
      " \" what? \" \n",
      " \" we should do something about this. \" \n",
      " \" what? \" \n",
      " \" we should do something abou\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type='openai-gpt' \\\n",
    "    --length=100 \\\n",
    "    --model_name_or_path='openai-gpt' \\\n",
    "    --temperature=0.5 \\\n",
    "    --stop_token=\"|\" \\\n",
    "    --prompt=\"GRAM 'Telegram Open Network' 1.807366053|\\\n",
    "LTC Litecoin 52.63|\\\n",
    "BNB 'Binance Coin' 17.78|\\\n",
    "FCT Factom 2.59|\\\n",
    "CVC Civic 0.037939729|\\\n",
    "BOTX botXcoin 0.015452991|\\\n",
    "ICN Iconomi 0.211692693|\\\n",
    "RHOC RChain 0.066656079|\\\n",
    "PAI2 'Project Pai' 0.01704574|\\\n",
    "QBIT Qubitica 30.48|\\\n",
    "BNK Bankera 0.000996052|\\\n",
    "RDD ReddCoin 0.000845153|\\\n",
    "MOF 'Molecular Future' 0.583074748|\\\n",
    "R Revain 0.048948372|\\\n",
    "AION Aion 0.066565099|\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that didn't produce cryptos at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Data is weird enough...\n",
    "\n",
    "In the debugging process I encountered names of existing crypto coins that are  weirder than ML could have randomly come up with, and are trading at non-zero $USD values….\n",
    "\n",
    "```\n",
    "MAY \"Theresa May Coin\" 0.000197574\n",
    "TSE \"Tattoocoin (Standard Edition)\" 0.000217536\n",
    "FLUZ 'Fluz Fluz' 0.022218693\n",
    "LALA 'LALA World' 0.017091302\n",
    "POE Po.et 0.00214985\n",
    "EMC2 Einsteinium 0.042217778\n",
    "FAT Fatcoin 0.018319265\n",
    "GBC2 'Gold Bits Coin' 0.022218693\n",
    "```\n",
    "\n",
    "Full list of my input cryptos in github here: https://github.com/aberke/city-visions/blob/master/week-2/data/cryptocurrencies_data.txt \n",
    "\n",
    "This is a classic example of why you should inspect your data before processing your data.  I should have realized the futility of this art project  from the start: I wasn’t going to make something weirder than the world of cryptofans had already invested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot\n",
    "\n",
    "[Chatbots](https://en.wikipedia.org/wiki/Chatbot) are conversational AI agents that can respond to text input. It's still ways away from a convincing conversation in general open-ended scenarios, but in certain applications chatbots are a big success, e.g. in the public services industry's online portals.\n",
    "\n",
    "`huggingface` again have released their pretrained models for chatbots based on transformers just a few months ago: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#79c5\n",
    "\n",
    "You can also use their online demo: https://convai.huggingface.co/persona/my-only-friend-is-a-dog-i-work-at-a-newspaper-my-father-used-to-be-a-butcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transfer-learning-conv-ai'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 96 (delta 0), reused 1 (delta 0), pack-reused 89\u001b[K\n",
      "Unpacking objects: 100% (96/96), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transfer-learning-conv-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_proc():\n",
    "    proc = subprocess.Popen([sys.executable, \n",
    "                             os.getcwd()+'/transfer-learning-conv-ai/interact.py'\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stdin=subprocess.PIPE,\n",
    "                            stderr=subprocess.DEVNULL\n",
    "                           )\n",
    "    pout = proc.stdout\n",
    "    pin = proc.stdin\n",
    "    \n",
    "    return proc,pout,pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc, cb1_pout, cb1_pin = chatbot_proc() # create a chatbot process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb1_proc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0xb3763de10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('cb1_proc')\n",
    "cb1_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb1_pin.write(b\"--temperature=1.1\\n\"), cb1_pin.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-faf8235f69fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb1_pout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(cb1_pout.readline().decode(\"--temperature=1.1\\n\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(cb1_pout.readline().decode(sys.stdout.encoding))\n",
    "# print(cb1_pout.readline().decode(\"--temperature=1.1\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_pin.write(b\"i'm doing mighty fine! and how are you?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))\n",
    "# print(cb1_pout.readline().decode(\"i'm doing mighty fine! and how are you?\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_pin.write(b\"no way! i'm also listening to music. what music are you listening to?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also quite funny to get it to talk to itself - it never get tired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_output = b\"i am listening to a lot of pop music\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: yeah, i know what you mean.\n",
      "A: what do you do for a living?\n",
      "B: i am a mechanic.\n",
      "A: i am a pilot.\n",
      "B: what do you do for work?\n",
      "A: i fix planes.\n",
      "B: what kind of planes do you have?\n",
      "A: do you have any hobbies?\n",
      "B: i like to listen to music.\n",
      "A: what kind of music do you like?\n"
     ]
    }
   ],
   "source": [
    "partyA = True\n",
    "for _ in range(10):\n",
    "    partyA = not partyA\n",
    "    cb1_pin.write(cb1_output), cb1_pin.flush();\n",
    "    cb1_output = cb1_pout.readline()[4:]\n",
    "    print(\"%s: %s\" % ('A' if partyA else 'B',\n",
    "          cb1_output[:-1].decode(sys.stdout.encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc.kill() # kill the chatbot process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try some different inputs\n",
    "* Advanced: Spin up another chatbot and have them talk to one another (by feeding the outputs across)\n",
    "* Advanced: Use a different underlying model than GPT-2 for your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's a wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
